{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import morph_kgc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"\"\"\n",
    "             [GTFS-Madrid-Bench]\n",
    "             mappings: mapping.csv.ttl\n",
    "         \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2023-04-21 23:31:22,152 | Parallelization is not supported for win32 when running as a library. If you need to speed up your data integration pipeline, please run through the command line.\n",
      "INFO | 2023-04-21 23:31:23,318 | 156 mapping rules retrieved.\n",
      "INFO | 2023-04-21 23:31:23,381 | Mapping partition with 81 groups generated.\n",
      "INFO | 2023-04-21 23:31:23,382 | Maximum number of rules within mapping group: 14.\n",
      "INFO | 2023-04-21 23:31:23,383 | Mappings processed in 1.229 seconds.\n",
      "INFO | 2023-04-21 23:31:24,507 | Number of triples generated in total: 2008.\n"
     ]
    }
   ],
   "source": [
    "g = morph_kgc.materialize(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = rdflib.Graph().parse('mapping.csv.ttl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "    PREFIX rml: <http://semweb.mmlab.be/ns/rml#>\n",
    "    \n",
    "    DELETE {\n",
    "        ?h rml:source ?source .\n",
    "    }\n",
    "    \n",
    "    INSERT {\n",
    "        ?h rml:source ?new_source .\n",
    "    }\n",
    "    \n",
    "    WHERE {\n",
    "        ?h rml:source ?source .\n",
    "        BIND(CONCAT(\".aux/\", ?source) AS ?new_source) .\n",
    "    }\n",
    "\"\"\"\n",
    "q_res = g.update(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".aux/data/STOPS.csv\n",
      ".aux/data/FREQUENCIES.csv\n",
      ".aux/data/SHAPES.csv\n",
      ".aux/data/SHAPES.csv\n",
      ".aux/data/ROUTES.csv\n",
      ".aux/data/CALENDAR_DATES.csv\n",
      ".aux/data/CALENDAR_DATES.csv\n",
      ".aux/data/CALENDAR.csv\n",
      ".aux/data/CALENDAR.csv\n",
      ".aux/data/TRIPS.csv\n",
      ".aux/data/AGENCY.csv\n",
      ".aux/data/STOP_TIMES.csv\n",
      ".aux/data/FEED_INFO.csv\n"
     ]
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "         PREFIX rml: <http://semweb.mmlab.be/ns/rml#>\n",
    "\n",
    "         SELECT ?source {\n",
    "            ?h rml:source ?source\n",
    "         }\n",
    "      \"\"\"\n",
    "\n",
    "q_res = g.query(q)\n",
    "\n",
    "for row in q_res:\n",
    "    print(row['source'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading mapping as graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=Nf7c36b50715e4c10a225f9252bc6771e (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = rdflib.Graph()\n",
    "mapping.parse('mapping.csv.ttl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "942"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/STOP_TIMES.csv\n",
      "data/TRIPS.csv\n",
      "data/ROUTES.csv\n",
      "data/AGENCY.csv\n",
      "data/STOPS.csv\n",
      "data/CALENDAR.csv\n",
      "data/CALENDAR.csv\n",
      "data/CALENDAR_DATES.csv\n",
      "data/CALENDAR_DATES.csv\n",
      "data/FEED_INFO.csv\n",
      "data/SHAPES.csv\n",
      "data/SHAPES.csv\n",
      "data/FREQUENCIES.csv\n"
     ]
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "         PREFIX rml: <http://semweb.mmlab.be/ns/rml#>\n",
    "\n",
    "         SELECT ?source {\n",
    "            ?h rml:source ?source\n",
    "         }\n",
    "      \"\"\"\n",
    "\n",
    "q_res = mapping.query(q)\n",
    "\n",
    "for row in q_res:\n",
    "    print(row['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sources = set([str(row['source']) for row in q_res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data/AGENCY.csv',\n",
       " 'data/CALENDAR.csv',\n",
       " 'data/CALENDAR_DATES.csv',\n",
       " 'data/FEED_INFO.csv',\n",
       " 'data/FREQUENCIES.csv',\n",
       " 'data/ROUTES.csv',\n",
       " 'data/SHAPES.csv',\n",
       " 'data/STOPS.csv',\n",
       " 'data/STOP_TIMES.csv',\n",
       " 'data/TRIPS.csv'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sources"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_dir(path: str, snapshot: str):\n",
    "    \"\"\"path: input data source path\n",
    "    snapshot: path of snapshot\n",
    "    \"\"\"\n",
    "    # load snapshot\n",
    "    if os.path.exists(snapshot):\n",
    "        new_version = False\n",
    "        # snapshot exists\n",
    "        with open(snapshot, 'rb') as f:\n",
    "            sp = pickle.load(file=f)\n",
    "    else:\n",
    "        new_version = True\n",
    "        # first version\n",
    "        sp = dict()\n",
    "    \n",
    "    # data dir\n",
    "    data_dir = os.fsencode(path) # TODO: quitar '/' si aparece al final\n",
    "    # new data dir\n",
    "    new_data_dir = os.fsencode(path + '_new')\n",
    "    # create temp dir for new data\n",
    "    if not os.path.exists(new_data_dir):\n",
    "        os.makedirs(new_data_dir)\n",
    "\n",
    "    for file in os.listdir(data_dir):\n",
    "        filename = os.fsdecode(path + '/' + file.decode(\"ascii\"))\n",
    "        # read dataframes\n",
    "        df_ds = pd.read_csv(filename, dtype=str) # source dataframe\n",
    "        df_sp = pd.DataFrame() if new_version else sp[filename] # snapshot dataframe\n",
    "        \n",
    "        # find differences (assumes that new data is only in df_datasource)\n",
    "        new_data = pd.concat([df_sp, df_ds]).drop_duplicates(keep=False)\n",
    "\n",
    "        # save new data to new_data_dir\n",
    "        new_file_path = os.fsdecode(path + '_new/' + file.decode(\"ascii\"))\n",
    "        new_data.to_csv(new_file_path, index=False)\n",
    "        if len(new_data) == 0:\n",
    "            print(\"No new data in %s, created file %s\" % (file.decode('ascii'), new_file_path))\n",
    "        else:\n",
    "            print(\"Saved new data to %s\" % (new_file_path))\n",
    "        \n",
    "        # save current snapshot = old + new\n",
    "        sp[filename] = pd.concat([df_sp, new_data]) # should not have duplicates\n",
    "    \n",
    "    # save snaphsot\n",
    "    with open(snapshot, 'wb') as f:\n",
    "        pickle.dump(obj=sp, file=f)\n",
    "        print(\"Saved snapshot to\", snapshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diff_dir('./data', 'snapshot.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kg(aux_data_path: str, mapping_file: str, snapshot_file: str, old_graph: rdflib.Graph):\n",
    "    \"\"\"mapping: path to mapping file\n",
    "    snapshot: path to snapshot file\n",
    "    old_graph: None or old version\n",
    "    \"\"\"\n",
    "    # load snapshot\n",
    "    if os.path.exists(snapshot_file):\n",
    "        new_version = False\n",
    "        # snapshot exists\n",
    "        with open(snapshot_file, 'rb') as f:\n",
    "            sp = pickle.load(file=f)\n",
    "    else:\n",
    "        new_version = True\n",
    "        # first version\n",
    "        sp = dict()\n",
    "    \n",
    "    # Read mapping\n",
    "    mapping_graph = rdflib.Graph().parse(mapping_file)\n",
    "    \n",
    "    # Extract sources from mapping\n",
    "    mapping_query = \"\"\"\n",
    "            PREFIX rml: <http://semweb.mmlab.be/ns/rml#>\n",
    "\n",
    "            SELECT ?source {\n",
    "                ?h rml:source ?source\n",
    "            }\n",
    "        \"\"\"\n",
    "    query_res = mapping_graph.query(mapping_query)\n",
    "    all_sources = set([str(row['source']) for row in query_res]) # Ignore duplicates\n",
    "\n",
    "    # Create auxiliary data directory\n",
    "    aux_data_path = os.fsencode(aux_data_path) # TODO: quitar '/' si aparece al final\n",
    "    # create temp dir for new data\n",
    "    if not os.path.exists(aux_data_path):\n",
    "        os.makedirs(aux_data_path)\n",
    "    \n",
    "    # Calculate diff between every new and old file\n",
    "    for source_file in all_sources:\n",
    "        # read dataframes\n",
    "        df_ds = pd.read_csv(source_file, dtype=str) # source dataframe\n",
    "        df_sp = pd.DataFrame() if new_version else sp[source_file] # snapshot dataframe\n",
    "        \n",
    "        # find differences (assumes that new data is only in df_datasource)\n",
    "        new_data = pd.concat([df_sp, df_ds]).drop_duplicates(keep=False)\n",
    "\n",
    "        # save new data to new_data_dir\n",
    "        new_file_path = aux_data_path.decode('utf-8') + '/' + source_file\n",
    "        \n",
    "        # Create directories for aux file\n",
    "        os.makedirs(os.path.dirname(new_file_path), exist_ok=True)\n",
    "\n",
    "        # Save aux file\n",
    "        new_data.to_csv(new_file_path, index=False)\n",
    "        if len(new_data) == 0:\n",
    "            print(\"No new data in %s, created empty file %s.\" % (source_file, new_file_path))\n",
    "        else:\n",
    "            print(\"Found new data in %s, saved to file %s.\" % (source_file, new_file_path))\n",
    "        \n",
    "        # save current snapshot = old + new\n",
    "        sp[source_file] = pd.concat([df_sp, new_data]) # should not have duplicates\n",
    "    \n",
    "    # Save snaphsot\n",
    "    with open(snapshot_file, 'wb') as f:\n",
    "        pickle.dump(obj=sp, file=f)\n",
    "        print(\"Saved snapshot to\", snapshot_file)\n",
    "    \n",
    "    # Change source paths from mapping\n",
    "    query_update = \"\"\"\n",
    "            PREFIX rml: <http://semweb.mmlab.be/ns/rml#>\n",
    "            \n",
    "            DELETE { ?h rml:source ?source }\n",
    "            INSERT { ?h rml:source ?new_source }\n",
    "            WHERE {\n",
    "                ?h rml:source ?source .\n",
    "                BIND(CONCAT(\".aux/\", ?source) AS ?new_source) .\n",
    "            }\n",
    "        \"\"\"\n",
    "    mapping_graph.update(query_update)\n",
    "    \n",
    "    # Save new mapping\n",
    "    new_mapping_file = aux_data_path.decode('utf-8') + '/.aux_' + mapping_file\n",
    "    mapping_graph.serialize(new_mapping_file)\n",
    "\n",
    "    config = \"[GTFS-Madrid-Bench]\\nmappings: %s\" % new_mapping_file\n",
    "    new_graph = morph_kgc.materialize(config)\n",
    "\n",
    "    # TODO: delete temp data dir?\n",
    "\n",
    "    # return old_graph + new_graph\n",
    "    return new_graph if old_graph is None else old_graph + new_graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2023-04-21 23:36:52,727 | Parallelization is not supported for win32 when running as a library. If you need to speed up your data integration pipeline, please run through the command line.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new data in data/FEED_INFO.csv, created empty file .aux/data/FEED_INFO.csv.\n",
      "No new data in data/SHAPES.csv, created empty file .aux/data/SHAPES.csv.\n",
      "No new data in data/STOP_TIMES.csv, created empty file .aux/data/STOP_TIMES.csv.\n",
      "No new data in data/CALENDAR_DATES.csv, created empty file .aux/data/CALENDAR_DATES.csv.\n",
      "Found new data in data/AGENCY.csv, saved to file .aux/data/AGENCY.csv.\n",
      "No new data in data/ROUTES.csv, created empty file .aux/data/ROUTES.csv.\n",
      "No new data in data/STOPS.csv, created empty file .aux/data/STOPS.csv.\n",
      "No new data in data/FREQUENCIES.csv, created empty file .aux/data/FREQUENCIES.csv.\n",
      "No new data in data/TRIPS.csv, created empty file .aux/data/TRIPS.csv.\n",
      "No new data in data/CALENDAR.csv, created empty file .aux/data/CALENDAR.csv.\n",
      "Saved snapshot to snapshot.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2023-04-21 23:36:53,914 | 156 mapping rules retrieved.\n",
      "INFO | 2023-04-21 23:36:53,970 | Mapping partition with 81 groups generated.\n",
      "INFO | 2023-04-21 23:36:53,971 | Maximum number of rules within mapping group: 14.\n",
      "INFO | 2023-04-21 23:36:53,971 | Mappings processed in 1.243 seconds.\n",
      "INFO | 2023-04-21 23:36:54,872 | Number of triples generated in total: 7.\n"
     ]
    }
   ],
   "source": [
    "g = load_kg(aux_data_path= '.aux', mapping_file='mapping.csv.ttl', snapshot_file='snapshot.pkl', old_graph=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2008"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add new triples\n",
    "\n",
    "New total should be >= previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2023-04-21 16:05:36,173 | Parallelization is not supported for win32 when running as a library. If you need to speed up your data integration pipeline, please run through the command line.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new data to ./data_new/AGENCY.csv\n",
      "No new data in CALENDAR.csv, created file ./data_new/CALENDAR.csv\n",
      "No new data in CALENDAR_DATES.csv, created file ./data_new/CALENDAR_DATES.csv\n",
      "No new data in FEED_INFO.csv, created file ./data_new/FEED_INFO.csv\n",
      "No new data in FREQUENCIES.csv, created file ./data_new/FREQUENCIES.csv\n",
      "No new data in ROUTES.csv, created file ./data_new/ROUTES.csv\n",
      "No new data in SHAPES.csv, created file ./data_new/SHAPES.csv\n",
      "No new data in STOPS.csv, created file ./data_new/STOPS.csv\n",
      "No new data in STOP_TIMES.csv, created file ./data_new/STOP_TIMES.csv\n",
      "No new data in TRIPS.csv, created file ./data_new/TRIPS.csv\n",
      "Saved snapshot to snapshot.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2023-04-21 16:05:37,418 | 156 mapping rules retrieved.\n",
      "INFO | 2023-04-21 16:05:37,480 | Mapping partition with 81 groups generated.\n",
      "INFO | 2023-04-21 16:05:37,481 | Maximum number of rules within mapping group: 14.\n",
      "INFO | 2023-04-21 16:05:37,482 | Mappings processed in 1.307 seconds.\n",
      "INFO | 2023-04-21 16:05:38,492 | Number of triples generated in total: 7.\n"
     ]
    }
   ],
   "source": [
    "g = load_kg(path='./data', mapping='mapping.csv.ttl', snapshot='snapshot.pkl', old_graph=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3 = \"\"\"\n",
    "         PREFIX gtfs: <http://vocab.gtfs.org/terms#>\n",
    "         PREFIX geo: <http://www.w3.org/2003/01/geo/wgs84_pos#>\n",
    "         PREFIX dct: <http://purl.org/dc/terms/>\n",
    "\n",
    "         SELECT * WHERE {\n",
    "             ?stop a gtfs:Stop . \n",
    "             ?stop gtfs:locationType ?location .\n",
    "             OPTIONAL { ?stop dct:description ?stopDescription . }\n",
    "             OPTIONAL { \n",
    "                 ?stop geo:lat ?stopLat . \n",
    "                 ?stop geo:long ?stopLong .\n",
    "             }\n",
    "             OPTIONAL {?stop gtfs:wheelchairAccessible ?wheelchairAccessible . }\n",
    "             FILTER (?location=<http://transport.linkeddata.es/resource/LocationType/2>)\n",
    "         }\n",
    "      \"\"\"\n",
    "\n",
    "q3_res = g.query(q3)\n",
    "\n",
    "for row in q3_res:\n",
    "    print(row['stop'], row['stopLat'], row['stopLong'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://transport.linkeddata.es/madrid/agency/00000000000000000001 https://www.crtm.es/billetes-y-tarifas\n",
      "http://transport.linkeddata.es/madrid/agency/00000000000000000002 https://www.crtm.es/billeaaaaaaaaaaaaaaa\n"
     ]
    }
   ],
   "source": [
    "q3 = \"\"\"\n",
    "         PREFIX gtfs: <http://vocab.gtfs.org/terms#>\n",
    "\n",
    "         SELECT ?agency ?url WHERE {\n",
    "            ?agency a gtfs:Agency.\n",
    "            ?agency gtfs:fareUrl ?url\n",
    "         }\n",
    "      \"\"\"\n",
    "\n",
    "q3_res = g.query(q3)\n",
    "\n",
    "for row in q3_res:\n",
    "    print(row['agency'], row['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snapshot inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('snapshot.pkl', 'rb') as f:\n",
    "    snapshot = pickle.load(file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency_id</th>\n",
       "      <th>agency_name</th>\n",
       "      <th>agency_url</th>\n",
       "      <th>agency_timezone</th>\n",
       "      <th>agency_lang</th>\n",
       "      <th>agency_phone</th>\n",
       "      <th>agency_fare_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000000000000000001</td>\n",
       "      <td>00000000000000000001</td>\n",
       "      <td>http://www.crtm.es</td>\n",
       "      <td>00000000000000000001</td>\n",
       "      <td>00000000000000000001</td>\n",
       "      <td>00000000000000000001</td>\n",
       "      <td>https://www.crtm.es/billetes-y-tarifas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000000000000000002</td>\n",
       "      <td>00000000000000000003</td>\n",
       "      <td>http://www.crtm.es</td>\n",
       "      <td>00000000000000000001</td>\n",
       "      <td>00000000000000000001</td>\n",
       "      <td>00000000000000000001</td>\n",
       "      <td>https://www.crtm.es/billeaaaaaaaaaaaaaaa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              agency_id           agency_name          agency_url  \\\n",
       "0  00000000000000000001  00000000000000000001  http://www.crtm.es   \n",
       "1  00000000000000000002  00000000000000000003  http://www.crtm.es   \n",
       "\n",
       "        agency_timezone           agency_lang          agency_phone  \\\n",
       "0  00000000000000000001  00000000000000000001  00000000000000000001   \n",
       "1  00000000000000000001  00000000000000000001  00000000000000000001   \n",
       "\n",
       "                            agency_fare_url  \n",
       "0    https://www.crtm.es/billetes-y-tarifas  \n",
       "1  https://www.crtm.es/billeaaaaaaaaaaaaaaa  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot['./data/AGENCY.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "- leer mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"a\": [1, 2, 3, 9], \"b\": [4, 5, 6, 10], \"c\": [6, 7, 8, 11]})\n",
    "df2 = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [6, 7, 8]})\n",
    "df3 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df1)\n",
    "display(df2)\n",
    "display(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find differences\n",
    "len(pd.concat([df1, df2]).drop_duplicates(keep=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3bf64417fad4ce5f95a5d445fa99ed58dabe0bfbbb23771aa975d1673e089270"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
