{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create graph with current version of morph_kgc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import morph_kgc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"\"\"\n",
    "             [GTFS-Madrid-Bench]\n",
    "             mappings: mapping.csv.ttl\n",
    "         \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2023-05-03 14:07:46,377 | Parallelization is not supported for win32 when running as a library. If you need to speed up your data integration pipeline, please run through the command line.\n",
      "INFO | 2023-05-03 14:07:47,628 | 156 mapping rules retrieved.\n",
      "INFO | 2023-05-03 14:07:47,693 | Mapping partition with 81 groups generated.\n",
      "INFO | 2023-05-03 14:07:47,694 | Maximum number of rules within mapping group: 14.\n",
      "INFO | 2023-05-03 14:07:47,694 | Mappings processed in 1.315 seconds.\n",
      "INFO | 2023-05-03 14:07:48,876 | Number of triples generated in total: 2001.\n"
     ]
    }
   ],
   "source": [
    "g = morph_kgc.materialize(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping file tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = rdflib.Graph().parse('mapping.csv.ttl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "942"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "        PREFIX rml: <http://semweb.mmlab.be/ns/rml#>\n",
    "        PREFIX sd: <https://w3id.org/okn/o/sd/>\n",
    "        PREFIX ql: <http://semweb.mmlab.be/ns/ql#>\n",
    "            \n",
    "        DELETE {\n",
    "            ?source rml:source ?source_file.\n",
    "            ?source ?r ?t.\n",
    "        }\n",
    "        INSERT {\n",
    "            #?source rml:source ?source_file. # TODO: remove this\n",
    "            ?source a rml:LogicalSource;\n",
    "                rml:source [\n",
    "                    a sd:DatasetSpecification;\n",
    "                    sd:name ?source_file;\n",
    "                    sd:hasDataTransformation [\n",
    "                        sd:hasSoftwareRequirements \"pandas>=1.1.0\";\n",
    "                        sd:hasSourceCode [\n",
    "                            sd:programmingLanguage \"Python3.9\";\n",
    "\t\t\t            ];\n",
    "\t\t            ];\n",
    "                ].\n",
    "        }\n",
    "        WHERE {\n",
    "            ?source rml:source ?source_file.\n",
    "        }\n",
    "      \"\"\"\n",
    "q_res = g.update(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1098"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/STOP_TIMES.csv\n",
      "data/TRIPS.csv\n",
      "data/ROUTES.csv\n",
      "data/AGENCY.csv\n",
      "data/STOPS.csv\n",
      "data/CALENDAR.csv\n",
      "data/CALENDAR.csv\n",
      "data/CALENDAR_DATES.csv\n",
      "data/CALENDAR_DATES.csv\n",
      "data/FEED_INFO.csv\n",
      "data/SHAPES.csv\n",
      "data/SHAPES.csv\n",
      "data/FREQUENCIES.csv\n"
     ]
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "         PREFIX rml: <http://semweb.mmlab.be/ns/rml#>\n",
    "\n",
    "         SELECT ?source {\n",
    "            ?h rml:source ?source\n",
    "         }\n",
    "      \"\"\"\n",
    "\n",
    "q_res = g.query(q)\n",
    "\n",
    "for row in q_res:\n",
    "    print(row['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.w3.org/1999/02/22-rdf-syntax-ns#type http://semweb.mmlab.be/ns/rml#LogicalSource\n",
      "http://semweb.mmlab.be/ns/rml#source data/STOP_TIMES.csv\n",
      "http://semweb.mmlab.be/ns/rml#referenceFormulation http://semweb.mmlab.be/ns/ql#CSV\n"
     ]
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "         PREFIX rml: <http://semweb.mmlab.be/ns/rml#>\n",
    "\n",
    "         SELECT ?r ?t {\n",
    "            :source_000 ?r ?t\n",
    "         }\n",
    "      \"\"\"\n",
    "\n",
    "q_res = g.query(q)\n",
    "\n",
    "for row in q_res:\n",
    "    print(row['r'], row['t'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental KG construction tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import incremental_kg as inc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aux to disk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2023-05-03 14:31:00,740 | Parallelization is not supported for win32 when running as a library. If you need to speed up your data integration pipeline, please run through the command line.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found new data in data/STOPS.csv, saved to file .aux/data/STOPS.csv.\n",
      "Found new data in data/FREQUENCIES.csv, saved to file .aux/data/FREQUENCIES.csv.\n",
      "Found new data in data/SHAPES.csv, saved to file .aux/data/SHAPES.csv.\n",
      "Found new data in data/ROUTES.csv, saved to file .aux/data/ROUTES.csv.\n",
      "Found new data in data/TRIPS.csv, saved to file .aux/data/TRIPS.csv.\n",
      "Found new data in data/AGENCY.csv, saved to file .aux/data/AGENCY.csv.\n",
      "Found new data in data/STOP_TIMES.csv, saved to file .aux/data/STOP_TIMES.csv.\n",
      "Found new data in data/CALENDAR.csv, saved to file .aux/data/CALENDAR.csv.\n",
      "Found new data in data/CALENDAR_DATES.csv, saved to file .aux/data/CALENDAR_DATES.csv.\n",
      "Found new data in data/FEED_INFO.csv, saved to file .aux/data/FEED_INFO.csv.\n",
      "Saved snapshot to snapshot.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2023-05-03 14:31:01,898 | 156 mapping rules retrieved.\n",
      "INFO | 2023-05-03 14:31:01,957 | Mapping partition with 81 groups generated.\n",
      "INFO | 2023-05-03 14:31:01,957 | Maximum number of rules within mapping group: 14.\n",
      "INFO | 2023-05-03 14:31:01,958 | Mappings processed in 1.218 seconds.\n",
      "INFO | 2023-05-03 14:31:03,008 | Number of triples generated in total: 2001.\n"
     ]
    }
   ],
   "source": [
    "g = inc.load_kg_aux_to_disk(\n",
    "    aux_data_path='.aux',\n",
    "    mapping_file='mapping.csv.ttl',\n",
    "    snapshot_file='snapshot.pkl',\n",
    "    old_graph=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2001"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add new triples\n",
    "\n",
    "New total should be >= previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2023-05-03 14:31:38,915 | Parallelization is not supported for win32 when running as a library. If you need to speed up your data integration pipeline, please run through the command line.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new data in data/STOPS.csv, created empty file .aux/data/STOPS.csv.\n",
      "No new data in data/FREQUENCIES.csv, created empty file .aux/data/FREQUENCIES.csv.\n",
      "No new data in data/SHAPES.csv, created empty file .aux/data/SHAPES.csv.\n",
      "No new data in data/ROUTES.csv, created empty file .aux/data/ROUTES.csv.\n",
      "No new data in data/TRIPS.csv, created empty file .aux/data/TRIPS.csv.\n",
      "Found new data in data/AGENCY.csv, saved to file .aux/data/AGENCY.csv.\n",
      "No new data in data/STOP_TIMES.csv, created empty file .aux/data/STOP_TIMES.csv.\n",
      "No new data in data/CALENDAR.csv, created empty file .aux/data/CALENDAR.csv.\n",
      "No new data in data/CALENDAR_DATES.csv, created empty file .aux/data/CALENDAR_DATES.csv.\n",
      "No new data in data/FEED_INFO.csv, created empty file .aux/data/FEED_INFO.csv.\n",
      "Saved snapshot to snapshot.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2023-05-03 14:31:40,069 | 156 mapping rules retrieved.\n",
      "INFO | 2023-05-03 14:31:40,128 | Mapping partition with 81 groups generated.\n",
      "INFO | 2023-05-03 14:31:40,129 | Maximum number of rules within mapping group: 14.\n",
      "INFO | 2023-05-03 14:31:40,130 | Mappings processed in 1.214 seconds.\n",
      "INFO | 2023-05-03 14:31:41,024 | Number of triples generated in total: 7.\n"
     ]
    }
   ],
   "source": [
    "g = inc.load_kg_aux_to_disk(\n",
    "    aux_data_path='.aux',\n",
    "    mapping_file='mapping.csv.ttl',\n",
    "    snapshot_file='snapshot.pkl',\n",
    "    old_graph=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2008"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aux to mem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new data in data/STOPS.csv, created empty dataframe.\n",
      "No new data in data/FEED_INFO.csv, created empty dataframe.\n",
      "No new data in data/SHAPES.csv, created empty dataframe.\n",
      "No new data in data/FREQUENCIES.csv, created empty dataframe.\n",
      "No new data in data/ROUTES.csv, created empty dataframe.\n",
      "No new data in data/CALENDAR_DATES.csv, created empty dataframe.\n",
      "No new data in data/CALENDAR.csv, created empty dataframe.\n",
      "No new data in data/STOP_TIMES.csv, created empty dataframe.\n",
      "No new data in data/AGENCY.csv, created empty dataframe.\n",
      "No new data in data/TRIPS.csv, created empty dataframe.\n",
      "Saved snapshot to snapshot.pkl\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "materialize() got an unexpected keyword argument 'python_source'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13740/3641642273.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m g = inc.load_kg_aux_to_mem(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mmapping_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mapping.csv.ttl'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msnapshot_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'snapshot.pkl'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     old_graph=None)\n",
      "\u001b[1;32mc:\\Users\\Jan\\Desktop\\I\\UPM\\MasterCD\\Masters Final Project\\incremental-kg\\src\\incremental_kg.py\u001b[0m in \u001b[0;36mload_kg_aux_to_mem\u001b[1;34m(mapping_file, snapshot_file, old_graph)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"[GTFS-Madrid-Bench]\\nmappings: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mnew_mapping_file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m     \u001b[0mnew_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmorph_kgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaterialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_data_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;31m# TODO: delete temp data dir?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: materialize() got an unexpected keyword argument 'python_source'"
     ]
    }
   ],
   "source": [
    "g = inc.load_kg_aux_to_mem(\n",
    "    mapping_file='mapping.csv.ttl',\n",
    "    snapshot_file='snapshot.pkl',\n",
    "    old_graph=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2001"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add new triples\n",
    "\n",
    "New total should be >= previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2023-05-03 14:31:38,915 | Parallelization is not supported for win32 when running as a library. If you need to speed up your data integration pipeline, please run through the command line.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new data in data/STOPS.csv, created empty file .aux/data/STOPS.csv.\n",
      "No new data in data/FREQUENCIES.csv, created empty file .aux/data/FREQUENCIES.csv.\n",
      "No new data in data/SHAPES.csv, created empty file .aux/data/SHAPES.csv.\n",
      "No new data in data/ROUTES.csv, created empty file .aux/data/ROUTES.csv.\n",
      "No new data in data/TRIPS.csv, created empty file .aux/data/TRIPS.csv.\n",
      "Found new data in data/AGENCY.csv, saved to file .aux/data/AGENCY.csv.\n",
      "No new data in data/STOP_TIMES.csv, created empty file .aux/data/STOP_TIMES.csv.\n",
      "No new data in data/CALENDAR.csv, created empty file .aux/data/CALENDAR.csv.\n",
      "No new data in data/CALENDAR_DATES.csv, created empty file .aux/data/CALENDAR_DATES.csv.\n",
      "No new data in data/FEED_INFO.csv, created empty file .aux/data/FEED_INFO.csv.\n",
      "Saved snapshot to snapshot.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2023-05-03 14:31:40,069 | 156 mapping rules retrieved.\n",
      "INFO | 2023-05-03 14:31:40,128 | Mapping partition with 81 groups generated.\n",
      "INFO | 2023-05-03 14:31:40,129 | Maximum number of rules within mapping group: 14.\n",
      "INFO | 2023-05-03 14:31:40,130 | Mappings processed in 1.214 seconds.\n",
      "INFO | 2023-05-03 14:31:41,024 | Number of triples generated in total: 7.\n"
     ]
    }
   ],
   "source": [
    "g = inc.load_kg_aux_to_mem(\n",
    "    mapping_file='mapping.csv.ttl',\n",
    "    snapshot_file='snapshot.pkl',\n",
    "    old_graph=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2008"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://transport.linkeddata.es/madrid/agency/00000000000000000001 https://www.crtm.es/billetes-y-tarifas\n",
      "http://transport.linkeddata.es/madrid/agency/00000000000000000002 https://www.crtm.es/billetes-y-tarifasaa\n"
     ]
    }
   ],
   "source": [
    "q3 = \"\"\"\n",
    "         PREFIX gtfs: <http://vocab.gtfs.org/terms#>\n",
    "\n",
    "         SELECT ?agency ?url WHERE {\n",
    "            ?agency a gtfs:Agency.\n",
    "            ?agency gtfs:fareUrl ?url\n",
    "         }\n",
    "      \"\"\"\n",
    "\n",
    "q3_res = g.query(q3)\n",
    "\n",
    "for row in q3_res:\n",
    "    print(row['agency'], row['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://transport.linkeddata.es/madrid/metro/stops/000000000000000000ho 929.0 889.0\n",
      "http://transport.linkeddata.es/madrid/metro/stops/000000000000000000dr 697.0 657.0\n",
      "http://transport.linkeddata.es/madrid/metro/stops/000000000000000000qt 750.0 710.0\n",
      "http://transport.linkeddata.es/madrid/metro/stops/000000000000000000e4 476.0 436.0\n",
      "http://transport.linkeddata.es/madrid/metro/stops/000000000000000000dz 716.0 676.0\n",
      "http://transport.linkeddata.es/madrid/metro/stops/000000000000000000tm 579.0 539.0\n",
      "http://transport.linkeddata.es/madrid/metro/stops/00000000000000000036 151.0 111.0\n",
      "http://transport.linkeddata.es/madrid/metro/stops/0000000000000000006o 231.0 191.0\n",
      "http://transport.linkeddata.es/madrid/metro/stops/000000000000000000xj 739.0 699.0\n",
      "http://transport.linkeddata.es/madrid/metro/stops/000000000000000000gv 441.0 401.0\n",
      "http://transport.linkeddata.es/madrid/metro/stops/000000000000000000lh 87.0 47.0\n",
      "http://transport.linkeddata.es/madrid/metro/stops/0000000000000000007q 830.0 790.0\n"
     ]
    }
   ],
   "source": [
    "q3 = \"\"\"\n",
    "         PREFIX gtfs: <http://vocab.gtfs.org/terms#>\n",
    "         PREFIX geo: <http://www.w3.org/2003/01/geo/wgs84_pos#>\n",
    "         PREFIX dct: <http://purl.org/dc/terms/>\n",
    "\n",
    "         SELECT * WHERE {\n",
    "             ?stop a gtfs:Stop . \n",
    "             ?stop gtfs:locationType ?location .\n",
    "             OPTIONAL { ?stop dct:description ?stopDescription . }\n",
    "             OPTIONAL { \n",
    "                 ?stop geo:lat ?stopLat . \n",
    "                 ?stop geo:long ?stopLong .\n",
    "             }\n",
    "             OPTIONAL {?stop gtfs:wheelchairAccessible ?wheelchairAccessible . }\n",
    "             FILTER (?location=<http://transport.linkeddata.es/resource/LocationType/2>)\n",
    "         }\n",
    "      \"\"\"\n",
    "\n",
    "q3_res = g.query(q3)\n",
    "\n",
    "for row in q3_res:\n",
    "    print(row['stop'], row['stopLat'], row['stopLong'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snapshot inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('snapshot.pkl', 'rb') as f:\n",
    "    snapshot = pickle.load(file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency_id</th>\n",
       "      <th>agency_name</th>\n",
       "      <th>agency_url</th>\n",
       "      <th>agency_timezone</th>\n",
       "      <th>agency_lang</th>\n",
       "      <th>agency_phone</th>\n",
       "      <th>agency_fare_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000000000000000001</td>\n",
       "      <td>00000000000000000001</td>\n",
       "      <td>http://www.crtm.es</td>\n",
       "      <td>00000000000000000001</td>\n",
       "      <td>00000000000000000001</td>\n",
       "      <td>00000000000000000001</td>\n",
       "      <td>https://www.crtm.es/billetes-y-tarifas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000000000000000002</td>\n",
       "      <td>00000000000000000001</td>\n",
       "      <td>http://www.crtm.es</td>\n",
       "      <td>00000000000000000001</td>\n",
       "      <td>00000000000000000001</td>\n",
       "      <td>00000000000000000001</td>\n",
       "      <td>https://www.crtm.es/billetes-y-tarifasaa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              agency_id           agency_name          agency_url  \\\n",
       "0  00000000000000000001  00000000000000000001  http://www.crtm.es   \n",
       "1  00000000000000000002  00000000000000000001  http://www.crtm.es   \n",
       "\n",
       "        agency_timezone           agency_lang          agency_phone  \\\n",
       "0  00000000000000000001  00000000000000000001  00000000000000000001   \n",
       "1  00000000000000000001  00000000000000000001  00000000000000000001   \n",
       "\n",
       "                            agency_fare_url  \n",
       "0    https://www.crtm.es/billetes-y-tarifas  \n",
       "1  https://www.crtm.es/billetes-y-tarifasaa  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot['data/AGENCY.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old code (delete) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_dir(path: str, snapshot: str):\n",
    "    \"\"\"path: input data source path\n",
    "    snapshot: path of snapshot\n",
    "    \"\"\"\n",
    "    # load snapshot\n",
    "    if os.path.exists(snapshot):\n",
    "        new_version = False\n",
    "        # snapshot exists\n",
    "        with open(snapshot, 'rb') as f:\n",
    "            sp = pickle.load(file=f)\n",
    "    else:\n",
    "        new_version = True\n",
    "        # first version\n",
    "        sp = dict()\n",
    "    \n",
    "    # data dir\n",
    "    data_dir = os.fsencode(path) # TODO: quitar '/' si aparece al final\n",
    "    # new data dir\n",
    "    new_data_dir = os.fsencode(path + '_new')\n",
    "    # create temp dir for new data\n",
    "    if not os.path.exists(new_data_dir):\n",
    "        os.makedirs(new_data_dir)\n",
    "\n",
    "    for file in os.listdir(data_dir):\n",
    "        filename = os.fsdecode(path + '/' + file.decode(\"ascii\"))\n",
    "        # read dataframes\n",
    "        df_ds = pd.read_csv(filename, dtype=str) # source dataframe\n",
    "        df_sp = pd.DataFrame() if new_version else sp[filename] # snapshot dataframe\n",
    "        \n",
    "        # find differences (assumes that new data is only in df_datasource)\n",
    "        new_data = pd.concat([df_sp, df_ds]).drop_duplicates(keep=False)\n",
    "\n",
    "        # save new data to new_data_dir\n",
    "        new_file_path = os.fsdecode(path + '_new/' + file.decode(\"ascii\"))\n",
    "        new_data.to_csv(new_file_path, index=False)\n",
    "        if len(new_data) == 0:\n",
    "            print(\"No new data in %s, created file %s\" % (file.decode('ascii'), new_file_path))\n",
    "        else:\n",
    "            print(\"Saved new data to %s\" % (new_file_path))\n",
    "        \n",
    "        # save current snapshot = old + new\n",
    "        sp[filename] = pd.concat([df_sp, new_data]) # should not have duplicates\n",
    "    \n",
    "    # save snaphsot\n",
    "    with open(snapshot, 'wb') as f:\n",
    "        pickle.dump(obj=sp, file=f)\n",
    "        print(\"Saved snapshot to\", snapshot)\n",
    "\n",
    "#diff_dir('./data', 'snapshot.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3bf64417fad4ce5f95a5d445fa99ed58dabe0bfbbb23771aa975d1673e089270"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
