{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import morph_kgc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"\"\"\n",
    "             [GTFS-Madrid-Bench]\n",
    "             mappings: mapping.csv.ttl\n",
    "         \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2023-04-21 19:11:01,868 | Parallelization is not supported for win32 when running as a library. If you need to speed up your data integration pipeline, please run through the command line.\n",
      "INFO | 2023-04-21 19:11:03,708 | 156 mapping rules retrieved.\n",
      "INFO | 2023-04-21 19:11:03,766 | Mapping partition with 81 groups generated.\n",
      "INFO | 2023-04-21 19:11:03,768 | Maximum number of rules within mapping group: 14.\n",
      "INFO | 2023-04-21 19:11:03,768 | Mappings processed in 1.891 seconds.\n",
      "INFO | 2023-04-21 19:11:04,789 | Number of triples generated in total: 2008.\n"
     ]
    }
   ],
   "source": [
    "g = morph_kgc.materialize(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def diff_dir(path: str, snapshot: str):\n",
    "    \"\"\"path: input data source path\n",
    "    snapshot: path of snapshot\n",
    "    \"\"\"\n",
    "    # current dir\n",
    "    directory = os.fsencode(path)\n",
    "\n",
    "    if os.path.exists(snapshot):\n",
    "        # snapshot exists\n",
    "        with open(snapshot, 'rb') as f:\n",
    "            sp = pickle.load(file=f)\n",
    "        # compare each file\n",
    "        for file in os.listdir(directory):\n",
    "            filename = path + '/' + os.fsdecode(file)\n",
    "            df_ds = pd.read_csv(filename, dtype=str)\n",
    "            df_sp = sp[filename]\n",
    "            # find differences (assumes that new data is only in df_datasource)\n",
    "            new_data = pd.concat([df_sp, df_ds]).drop_duplicates(keep=False)\n",
    "\n",
    "            new_dir_path = path + '_new'            \n",
    "            if not os.path.exists(new_dir_path):\n",
    "                    os.makedirs(new_dir_path)\n",
    "\n",
    "            # save newdata to _new dir\n",
    "            new_file_path = new_dir_path + '/' + os.fsdecode(file)\n",
    "            new_data.to_csv(new_file_path, index=False)\n",
    "            if len(new_data) == 0:\n",
    "                print(\"No new data in %s, created file %s\" % (file.decode('ascii'), new_file_path))\n",
    "            else:\n",
    "                print(\"Saved new data to %s\" % (new_file_path))\n",
    "    else:\n",
    "        # create snapshot\n",
    "        sp = dict()\n",
    "        for file in os.listdir(directory):\n",
    "            filename = path + '/' + os.fsdecode(file)\n",
    "            df = pd.read_csv(filename, dtype=str)\n",
    "            sp[filename] = df\n",
    "        # save snaphsot\n",
    "        with open(snapshot, 'wb') as f:\n",
    "            pickle.dump(obj=sp, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_dir(path: str, snapshot: str):\n",
    "    \"\"\"path: input data source path\n",
    "    snapshot: path of snapshot\n",
    "    \"\"\"\n",
    "    # load snapshot\n",
    "    if os.path.exists(snapshot):\n",
    "        new_version = False\n",
    "        # snapshot exists\n",
    "        with open(snapshot, 'rb') as f:\n",
    "            sp = pickle.load(file=f)\n",
    "    else:\n",
    "        new_version = True\n",
    "        # first version\n",
    "        sp = dict()\n",
    "    \n",
    "    # data dir\n",
    "    data_dir = os.fsencode(path) # TODO: quitar '/' si aparece al final\n",
    "    # new data dir\n",
    "    new_data_dir = os.fsencode(path + '_new')\n",
    "    # create temp dir for new data\n",
    "    if not os.path.exists(new_data_dir):\n",
    "        os.makedirs(new_data_dir)\n",
    "\n",
    "    for file in os.listdir(data_dir):\n",
    "        filename = os.fsdecode(path + '/' + file.decode(\"ascii\"))\n",
    "        # read dataframes\n",
    "        df_ds = pd.read_csv(filename, dtype=str) # source dataframe\n",
    "        df_sp = pd.DataFrame() if new_version else sp[filename] # snapshot dataframe\n",
    "        \n",
    "        # find differences (assumes that new data is only in df_datasource)\n",
    "        new_data = pd.concat([df_sp, df_ds]).drop_duplicates(keep=False)\n",
    "\n",
    "        # save new data to new_data_dir\n",
    "        new_file_path = os.fsdecode(path + '_new/' + file.decode(\"ascii\"))\n",
    "        new_data.to_csv(new_file_path, index=False)\n",
    "        if len(new_data) == 0:\n",
    "            print(\"No new data in %s, created file %s\" % (file.decode('ascii'), new_file_path))\n",
    "        else:\n",
    "            print(\"Saved new data to %s\" % (new_file_path))\n",
    "        \n",
    "        # save current snapshot = old + new\n",
    "        sp[filename] = pd.concat([df_sp, new_data]) # should not have duplicates\n",
    "    \n",
    "    # save snaphsot\n",
    "    with open(snapshot, 'wb') as f:\n",
    "        pickle.dump(obj=sp, file=f)\n",
    "        print(\"Saved snapshot to\", snapshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diff_dir('./data', 'snapshot.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "def load_kg(path: str, mapping: str, snapshot: str, old_graph: rdflib.Graph):\n",
    "    \"\"\"path: input data source (directory)\n",
    "    mapping: path to mapping file\n",
    "    snapshot: path to snapshot file\n",
    "    old_graph: None or old version\n",
    "    \"\"\"\n",
    "    diff_dir(path, snapshot)\n",
    "    \n",
    "    path_base_name = os.path.basename(path)\n",
    "    new_path = path_base_name + '_new/'\n",
    "    path_base_name += '/'\n",
    "    \n",
    "    with open(mapping, 'r') as f:\n",
    "        mapping_lines = f.readlines()\n",
    "        mapping_lines = [line.replace(path_base_name, new_path) for line in mapping_lines]\n",
    "    \n",
    "    new_mapping_file = '.new_' + mapping\n",
    "    with open(new_mapping_file, 'w+') as f:\n",
    "        f.writelines(mapping_lines)\n",
    "\n",
    "    config = \"[GTFS-Madrid-Bench]\\nmappings: %s\" % new_mapping_file\n",
    "    new_graph = morph_kgc.materialize(config)\n",
    "\n",
    "    # TODO: delete temp data dir?\n",
    "\n",
    "    # return old_graph + new_graph\n",
    "    return new_graph if old_graph is None else old_graph + new_graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2023-04-21 16:04:58,762 | Parallelization is not supported for win32 when running as a library. If you need to speed up your data integration pipeline, please run through the command line.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new data to ./data_new/AGENCY.csv\n",
      "Saved new data to ./data_new/CALENDAR.csv\n",
      "Saved new data to ./data_new/CALENDAR_DATES.csv\n",
      "Saved new data to ./data_new/FEED_INFO.csv\n",
      "Saved new data to ./data_new/FREQUENCIES.csv\n",
      "Saved new data to ./data_new/ROUTES.csv\n",
      "Saved new data to ./data_new/SHAPES.csv\n",
      "Saved new data to ./data_new/STOPS.csv\n",
      "Saved new data to ./data_new/STOP_TIMES.csv\n",
      "Saved new data to ./data_new/TRIPS.csv\n",
      "Saved snapshot to snapshot.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2023-04-21 16:04:59,995 | 156 mapping rules retrieved.\n",
      "INFO | 2023-04-21 16:05:00,058 | Mapping partition with 81 groups generated.\n",
      "INFO | 2023-04-21 16:05:00,059 | Maximum number of rules within mapping group: 14.\n",
      "INFO | 2023-04-21 16:05:00,059 | Mappings processed in 1.296 seconds.\n",
      "INFO | 2023-04-21 16:05:01,174 | Number of triples generated in total: 2001.\n"
     ]
    }
   ],
   "source": [
    "g = load_kg(path='./data', mapping='mapping.csv.ttl', snapshot='snapshot.pkl', old_graph=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2001"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add new triples\n",
    "\n",
    "New total should be >= previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2023-04-21 16:05:36,173 | Parallelization is not supported for win32 when running as a library. If you need to speed up your data integration pipeline, please run through the command line.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new data to ./data_new/AGENCY.csv\n",
      "No new data in CALENDAR.csv, created file ./data_new/CALENDAR.csv\n",
      "No new data in CALENDAR_DATES.csv, created file ./data_new/CALENDAR_DATES.csv\n",
      "No new data in FEED_INFO.csv, created file ./data_new/FEED_INFO.csv\n",
      "No new data in FREQUENCIES.csv, created file ./data_new/FREQUENCIES.csv\n",
      "No new data in ROUTES.csv, created file ./data_new/ROUTES.csv\n",
      "No new data in SHAPES.csv, created file ./data_new/SHAPES.csv\n",
      "No new data in STOPS.csv, created file ./data_new/STOPS.csv\n",
      "No new data in STOP_TIMES.csv, created file ./data_new/STOP_TIMES.csv\n",
      "No new data in TRIPS.csv, created file ./data_new/TRIPS.csv\n",
      "Saved snapshot to snapshot.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2023-04-21 16:05:37,418 | 156 mapping rules retrieved.\n",
      "INFO | 2023-04-21 16:05:37,480 | Mapping partition with 81 groups generated.\n",
      "INFO | 2023-04-21 16:05:37,481 | Maximum number of rules within mapping group: 14.\n",
      "INFO | 2023-04-21 16:05:37,482 | Mappings processed in 1.307 seconds.\n",
      "INFO | 2023-04-21 16:05:38,492 | Number of triples generated in total: 7.\n"
     ]
    }
   ],
   "source": [
    "g = load_kg(path='./data', mapping='mapping.csv.ttl', snapshot='snapshot.pkl', old_graph=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3 = \"\"\"\n",
    "         PREFIX gtfs: <http://vocab.gtfs.org/terms#>\n",
    "         PREFIX geo: <http://www.w3.org/2003/01/geo/wgs84_pos#>\n",
    "         PREFIX dct: <http://purl.org/dc/terms/>\n",
    "\n",
    "         SELECT * WHERE {\n",
    "             ?stop a gtfs:Stop . \n",
    "             ?stop gtfs:locationType ?location .\n",
    "             OPTIONAL { ?stop dct:description ?stopDescription . }\n",
    "             OPTIONAL { \n",
    "                 ?stop geo:lat ?stopLat . \n",
    "                 ?stop geo:long ?stopLong .\n",
    "             }\n",
    "             OPTIONAL {?stop gtfs:wheelchairAccessible ?wheelchairAccessible . }\n",
    "             FILTER (?location=<http://transport.linkeddata.es/resource/LocationType/2>)\n",
    "         }\n",
    "      \"\"\"\n",
    "\n",
    "q3_res = g.query(q3)\n",
    "\n",
    "for row in q3_res:\n",
    "    print(row['stop'], row['stopLat'], row['stopLong'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://transport.linkeddata.es/madrid/agency/00000000000000000001 https://www.crtm.es/billetes-y-tarifas\n",
      "http://transport.linkeddata.es/madrid/agency/00000000000000000002 https://www.crtm.es/billeaaaaaaaaaaaaaaa\n"
     ]
    }
   ],
   "source": [
    "q3 = \"\"\"\n",
    "         PREFIX gtfs: <http://vocab.gtfs.org/terms#>\n",
    "\n",
    "         SELECT ?agency ?url WHERE {\n",
    "            ?agency a gtfs:Agency.\n",
    "            ?agency gtfs:fareUrl ?url\n",
    "         }\n",
    "      \"\"\"\n",
    "\n",
    "q3_res = g.query(q3)\n",
    "\n",
    "for row in q3_res:\n",
    "    print(row['agency'], row['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snapshot inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('snapshot.pkl', 'rb') as f:\n",
    "    snapshot = pickle.load(file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency_id</th>\n",
       "      <th>agency_name</th>\n",
       "      <th>agency_url</th>\n",
       "      <th>agency_timezone</th>\n",
       "      <th>agency_lang</th>\n",
       "      <th>agency_phone</th>\n",
       "      <th>agency_fare_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000000000000000001</td>\n",
       "      <td>00000000000000000001</td>\n",
       "      <td>http://www.crtm.es</td>\n",
       "      <td>00000000000000000001</td>\n",
       "      <td>00000000000000000001</td>\n",
       "      <td>00000000000000000001</td>\n",
       "      <td>https://www.crtm.es/billetes-y-tarifas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000000000000000002</td>\n",
       "      <td>00000000000000000003</td>\n",
       "      <td>http://www.crtm.es</td>\n",
       "      <td>00000000000000000001</td>\n",
       "      <td>00000000000000000001</td>\n",
       "      <td>00000000000000000001</td>\n",
       "      <td>https://www.crtm.es/billeaaaaaaaaaaaaaaa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              agency_id           agency_name          agency_url  \\\n",
       "0  00000000000000000001  00000000000000000001  http://www.crtm.es   \n",
       "1  00000000000000000002  00000000000000000003  http://www.crtm.es   \n",
       "\n",
       "        agency_timezone           agency_lang          agency_phone  \\\n",
       "0  00000000000000000001  00000000000000000001  00000000000000000001   \n",
       "1  00000000000000000001  00000000000000000001  00000000000000000001   \n",
       "\n",
       "                            agency_fare_url  \n",
       "0    https://www.crtm.es/billetes-y-tarifas  \n",
       "1  https://www.crtm.es/billeaaaaaaaaaaaaaaa  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot['./data/AGENCY.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "- leer mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"a\": [1, 2, 3, 9], \"b\": [4, 5, 6, 10], \"c\": [6, 7, 8, 11]})\n",
    "df2 = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [6, 7, 8]})\n",
    "df3 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df1)\n",
    "display(df2)\n",
    "display(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find differences\n",
    "len(pd.concat([df1, df2]).drop_duplicates(keep=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3bf64417fad4ce5f95a5d445fa99ed58dabe0bfbbb23771aa975d1673e089270"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
